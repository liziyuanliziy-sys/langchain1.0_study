{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d896f6e",
   "metadata": {},
   "source": [
    "### 1 基本用法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ef36d",
   "metadata": {},
   "source": [
    "#### 1.1 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d416d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U \"langchain[deepseek]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ce727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用init_chat_model 或者 Model class 都可以，这里使用Model class\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "model = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\"\n",
    ")\n",
    "\n",
    "result = model.invoke(\"你好，请你详细介绍下你自己\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372621ff",
   "metadata": {},
   "source": [
    "### 2 模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c036798",
   "metadata": {},
   "source": [
    "常见参数如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2031ef2",
   "metadata": {},
   "source": [
    "model -> string , require 你想使用的模型名称"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a3eb8",
   "metadata": {},
   "source": [
    "api_key -> string 模型的api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524260cb",
   "metadata": {},
   "source": [
    "temperature -> number 控制模型输出的随机性，数值越高，创造性越高，数值越低，则越确定。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488307e5",
   "metadata": {},
   "source": [
    "timeout -> number 等待模型响应的最大时间，以秒为单位，超时后取消请求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe4732",
   "metadata": {},
   "source": [
    "max_tokens -> number 最大令牌数 限制响应的token总数，控制输出长度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d020f",
   "metadata": {},
   "source": [
    "max_retries -> number 最大重试次数。系统在请求因网络超时或速率限制等问题失败时，最多重试的次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f7e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0,\n",
    "    timeout=1000,\n",
    "    max_tokens=1000,\n",
    "    max_retries=2\n",
    ")\n",
    "model.invoke(\"请你详细介绍下你自己\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e262d317",
   "metadata": {},
   "source": [
    "### 3 调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5a8ce3",
   "metadata": {},
   "source": [
    "#### 3.1 invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb89a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用模型最直接的方式是使用 invoke() ，传入单个消息或消息列表。\n",
    "response = model.invoke(\"你好\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecbab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "conversation = [\n",
    "    SystemMessage(\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(\"Translate: I love programming.\"),\n",
    "    AIMessage(\"J'adore la programmation.\"),\n",
    "    HumanMessage(\"Translate: I love building applications.\")\n",
    "]\n",
    "\n",
    "response = model.invoke(conversation)\n",
    "print(response)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd373f",
   "metadata": {},
   "source": [
    "#### 3.2 stream 流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e82c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本文本流\n",
    "for chunk in model.stream(\"你好，两句话介绍下自己\"):\n",
    "    print(chunk.text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c6a901",
   "metadata": {},
   "source": [
    "#### 3.3 批处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3818c175",
   "metadata": {},
   "source": [
    "将一组独立的请求批量提交给模型，可以显著提高性能并降低成本，因为处理可以并行进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900526d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = model.batch([\n",
    "    \"解释下机器学习\",\n",
    "    \"解释下深度学习\",\n",
    "    \"解释下强化学习\"\n",
    "])\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cacc70",
   "metadata": {},
   "source": [
    "默认情况下， batch() 只会返回整个批次的最终输出。如果你希望在每个输入生成完成后接收对应的输出，可以使用 batch_as_completed() 进行流式传输："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db62d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for response in model.batch_as_completed([\n",
    "    \"解释下机器学习\",\n",
    "    \"解释下深度学习\",\n",
    "    \"解释下强化学习\"\n",
    "]):\n",
    "    print(response)\n",
    "# 使用 batch_as_completed() 时，结果可能会无序到达。每个结果都包含输入索引，以便匹配并根据需要重建原始顺序。\n",
    "# 不知道为何其实还是一次性返回的。官方是这么说的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeffc39",
   "metadata": {},
   "source": [
    "在使用 batch() 或 batch_as_completed() 处理大量输入时，您可能希望控制最大并行调用次数。这可以通过在 RunnableConfig 字典中设置 max_concurrency 属性来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.batch(\n",
    "    [\n",
    "    \"解释下机器学习\",\n",
    "    \"解释下深度学习\",\n",
    "    \"解释下强化学习\"\n",
    "],\n",
    "    config={\n",
    "        'max_concurrency': 5,  # 限制为5个并发。\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511212ed",
   "metadata": {},
   "source": [
    "### 4 工具调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf868de",
   "metadata": {},
   "source": [
    "工具调用需要使用bind_tools绑定工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(city):\n",
    "    \"\"\"用来查询city天气\"\"\"\n",
    "    return f\"{city}的天气多云！\"\n",
    "\n",
    "llm_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "response = llm_with_tools.invoke(\"今天上海的天气如何\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2967e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool_call in response.tool_calls:\n",
    "    print(f\"Tool: {tool_call['name']}\")\n",
    "    print(f\"Args: {tool_call['args']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528f27eb",
   "metadata": {},
   "source": [
    "#### 4.1 完整的工具执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = model.bind_tools([get_weather])\n",
    "messages = [{\"role\":\"user\", \"content\":\"帮我查一下上海的天气\"}]\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "\n",
    "messages.append(ai_msg)\n",
    "\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    result = get_weather.invoke(tool_call)\n",
    "    messages.append(result)\n",
    "\n",
    "final_response = llm_with_tools.invoke(messages)\n",
    "final_response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9cb9ad",
   "metadata": {},
   "source": [
    "#### 4.2 强制工具调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce82ff",
   "metadata": {},
   "source": [
    "默认情况下，模型有自由根据用户输入选择使用哪个绑定工具。然而，你可能希望强制选择某个工具，确保模型使用特定的工具或来自给定列表的任何工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89321393",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tools = model.bind_tools([get_weather], tool_choice=\"any\") # 任何工具\n",
    "model_with_tools = model.bind_tools([get_weather], tool_choice=\"get_weather\") # 特定工具"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935474e4",
   "metadata": {},
   "source": [
    "#### 4.3 并行工具调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af4deb",
   "metadata": {},
   "source": [
    "许多模型在适当时支持并行调用多个工具。这允许模型同时从不同来源收集信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf3d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "response = model_with_tools.invoke(\"请问上海和北京的天气怎么样\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf7db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for tool_call in response.tool_calls:\n",
    "    result = get_weather.invoke(tool_call)\n",
    "    results.append(result)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 默认开启并行工具调用，如果禁用，则如下设置\n",
    "model.bind_tools([get_weather], parallel_tool_calls=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc8a9bb",
   "metadata": {},
   "source": [
    "#### 4.4 流式工具调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31937cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in model_with_tools.stream(\n",
    "    \"上海和北京的天气怎么样?\"\n",
    "):\n",
    "    # 工具调用片段逐步到达\n",
    "    for tool_chunk in chunk.tool_call_chunks:\n",
    "        # 海象运算符\n",
    "        if name := tool_chunk.get(\"name\"):\n",
    "            print(f\"Tool: {name}\")\n",
    "        if id_ := tool_chunk.get(\"id\"):\n",
    "            print(f\"ID: {id_}\")\n",
    "        if args := tool_chunk.get(\"args\"):\n",
    "            print(f\"Args: {args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a33a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "gathered = None\n",
    "for chunk in model_with_tools.stream(\"北京天气如何?\"):\n",
    "    gathered = chunk if gathered is None else gathered + chunk\n",
    "    print(gathered.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e6f34",
   "metadata": {},
   "source": [
    "### 5 结构化输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b2237",
   "metadata": {},
   "source": [
    "#### 5.1 Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f3b6e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='西虹市首富' year=2018 director='闫非' rating=6.6\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: str = Field(..., description=\"The title of the movie\")\n",
    "    year: int = Field(..., description=\"The year the movie was released\")\n",
    "    director: str = Field(..., description=\"The director of the movie\")\n",
    "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "# 绑定格式。\n",
    "model_with_structure = model.with_structured_output(Movie)\n",
    "response = model_with_structure.invoke(\"提供西虹市首富的电视剧介绍\")\n",
    "print(response)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923902a",
   "metadata": {},
   "source": [
    "#### 5.2 TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31dc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '西虹市首富', 'year': 2018, 'director': '闫非', 'rating': 6.6}\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "class MovieDict(TypedDict):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: Annotated[str, ..., \"The title of the movie\"]\n",
    "    year: Annotated[int, ..., \"The year the movie was released\"]\n",
    "    director: Annotated[str, ..., \"The director of the movie\"]\n",
    "    rating: Annotated[float, ..., \"The movie's rating out of 10\"]\n",
    "\n",
    "model_with_structure = model.with_structured_output(MovieDict)\n",
    "response = model_with_structure.invoke(\"提供西虹市首富的电视剧介绍\")\n",
    "print(response)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc511f6",
   "metadata": {},
   "source": [
    "### 6 速率限制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb9212",
   "metadata": {},
   "source": [
    "许多聊天模型提供商对在一定时间内的调用次数设有限制。如果达到速率限制，通常会收到提供商返回的速率限制错误响应，您需要等待一段时间后再发起请求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59deef0b",
   "metadata": {},
   "source": [
    "为了帮助管理速率限制，聊天模型集成接受一个 rate_limiter 参数，该参数在初始化时提供，用于控制请求的发送速率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae6a89a",
   "metadata": {},
   "source": [
    "LangChain 内置（可选） InMemoryRateLimiter 。该限制器是线程安全的，可以被同一进程中的多个线程共享。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=0.1,  # 每10秒请求一次  1 / requests_per_second = 请求间隔时间\n",
    "    check_every_n_seconds=0.1,  # 每100毫秒检查一次  检查是否允许发送请求的频率\n",
    "    max_bucket_size=10,  # 最大突发10个请求. 控制最大突发请求量\n",
    ")\n",
    "# 主要有些模型，有着请求数量速率限制和并发限制。\n",
    "model = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    rate_limiter=rate_limiter  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe046b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
